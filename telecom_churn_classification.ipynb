{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ravali0726/mihir/blob/main/telecom_churn_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Overview:\n",
        "* \"The Orange Telecom's Churn Dataset, which consists of cleaned customer activity data (features), along with a churn label specifying whether a customer canceled the subscription, will be used to develop predictive models.\"\n",
        "\n",
        "Objectives:\n",
        "* Build a classification model to predict customer churn, which is a binary label\n",
        "* Identify key drivers of customer churn and customer retention\n",
        "\n",
        "Success criteria:\n",
        "* Identify most of the customers that canceled the subscription (recall metric)\n",
        "\n",
        "Link to data:\n",
        "* https://www.kaggle.com/datasets/mnassrib/telecom-churn-datasets?select=churn-bigml-80.csv"
      ],
      "metadata": {
        "id": "KBKqVflwcW0S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Imports"
      ],
      "metadata": {
        "id": "oZ4cSdRPcW0c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the read-only \"../input/\" directory\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n",
        "\n",
        "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n",
        "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-09-03T20:42:59.464739Z",
          "iopub.execute_input": "2022-09-03T20:42:59.465168Z",
          "iopub.status.idle": "2022-09-03T20:42:59.473898Z",
          "shell.execute_reply.started": "2022-09-03T20:42:59.465106Z",
          "shell.execute_reply": "2022-09-03T20:42:59.473161Z"
        },
        "trusted": true,
        "id": "06PryvGmcW0d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime\n",
        "ts = datetime.now() # Notebook runtime\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "pd.set_option('display.max_columns', 100)\n",
        "\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.feature_selection import chi2\n",
        "from scipy.stats import ttest_ind\n",
        "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
        "from sklearn.model_selection import train_test_split, KFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn import metrics\n",
        "\n",
        "from imblearn.over_sampling import SMOTENC\n",
        "from imblearn.under_sampling import RandomUnderSampler, TomekLinks\n",
        "\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from lightgbm import LGBMClassifier"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-09-03T20:42:59.475796Z",
          "iopub.execute_input": "2022-09-03T20:42:59.476782Z",
          "iopub.status.idle": "2022-09-03T20:42:59.490742Z",
          "shell.execute_reply.started": "2022-09-03T20:42:59.476747Z",
          "shell.execute_reply": "2022-09-03T20:42:59.489566Z"
        },
        "trusted": true,
        "id": "03umhVWhcW0g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load Data"
      ],
      "metadata": {
        "id": "rxFDzXHEcW0i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/kaggle/input/telecom-churn-datasets/churn-bigml-80.csv')\n",
        "print(df.shape)\n",
        "df.head()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-09-03T20:42:59.492637Z",
          "iopub.execute_input": "2022-09-03T20:42:59.493064Z",
          "iopub.status.idle": "2022-09-03T20:42:59.532651Z",
          "shell.execute_reply.started": "2022-09-03T20:42:59.493022Z",
          "shell.execute_reply": "2022-09-03T20:42:59.531586Z"
        },
        "trusted": true,
        "id": "HMiRQkxccW0j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(*df.columns, sep='\\n')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-09-03T20:42:59.53497Z",
          "iopub.execute_input": "2022-09-03T20:42:59.53531Z",
          "iopub.status.idle": "2022-09-03T20:42:59.54223Z",
          "shell.execute_reply.started": "2022-09-03T20:42:59.535279Z",
          "shell.execute_reply": "2022-09-03T20:42:59.540968Z"
        },
        "trusted": true,
        "id": "0dhSWie2cW0k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Understand Data"
      ],
      "metadata": {
        "id": "Ucss9bNGcW0m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def df_info():\n",
        "    \"\"\"Summary stats and other info about the data,\n",
        "    like number of nulls, data types, unique values, and number of outliers\"\"\"\n",
        "\n",
        "    # Continuous data outliers using IQR\n",
        "    continuous_df = df.select_dtypes(include=[float, int])\n",
        "    Q1 = continuous_df.quantile(0.25)\n",
        "    Q3 = continuous_df.quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    outliers = ((continuous_df < (Q1 - 1.5 * IQR)) | (continuous_df > (Q3 + 1.5 * IQR))).sum()\n",
        "    outliers = outliers.to_frame(name='outliers')\n",
        "\n",
        "    # Summary stats for continuous data\n",
        "    descr = continuous_df.describe().T\n",
        "\n",
        "    # Nulls, data types, and unique values\n",
        "    nulls = df.isna().sum().to_frame(name='nulls')\n",
        "    dtypes = df.dtypes.to_frame(name='dtype')\n",
        "    nuniques = df.nunique().to_frame(name='nunique')\n",
        "    info = pd.concat([nulls, dtypes, nuniques], axis=1)\n",
        "\n",
        "    # Left join continuous data\n",
        "    info = info.merge(outliers, left_index=True, right_index=True, how='left')\n",
        "    info = info.merge(descr, left_index=True, right_index=True, how='left')\n",
        "    return info\n",
        "\n",
        "df_info()"
      ],
      "metadata": {
        "scrolled": true,
        "execution": {
          "iopub.status.busy": "2022-09-03T20:42:59.543597Z",
          "iopub.execute_input": "2022-09-03T20:42:59.544222Z",
          "iopub.status.idle": "2022-09-03T20:42:59.656249Z",
          "shell.execute_reply.started": "2022-09-03T20:42:59.544078Z",
          "shell.execute_reply": "2022-09-03T20:42:59.655092Z"
        },
        "trusted": true,
        "id": "FpCFOft8cW0o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prepare Data"
      ],
      "metadata": {
        "id": "bJa1EhNxcW0p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_data(df=df):\n",
        "    # Shuffle the data bc idk how it was generated and if it's sorted\n",
        "    df = (df\n",
        "          .sample(frac=1)\n",
        "          .reset_index(drop=True)\n",
        "         )\n",
        "\n",
        "    # Fix data types\n",
        "    df['Churn'] = df['Churn'].astype('uint8') # boolean --> integer\n",
        "    yesno_func = lambda x: 1 if x == 'Yes' else (0 if x == 'No' else None) # yes/no --> 1/0\n",
        "    df['International plan'] = df['International plan'].apply(yesno_func).astype('uint8')\n",
        "    df['Voice mail plan'] = df['Voice mail plan'].apply(yesno_func).astype('uint8')\n",
        "\n",
        "    # One-hot encode 'State' and 'Area code'\n",
        "    df = pd.get_dummies(df\n",
        "                        , columns=['State', 'Area code']\n",
        "                        , prefix={'State': 'State'\n",
        "                                  , 'Area code': 'Area_code'\n",
        "                                 }\n",
        "                       )\n",
        "\n",
        "    # Feature engineering: flag if the customer has both 'International plan' and 'Voice mail plan'\n",
        "    df['intl_and_vm_plan'] = df['International plan'] * df['Voice mail plan']\n",
        "\n",
        "    # Feature engineering: total and percent of total for 'calls', 'minutes', and 'charge' columns\n",
        "    cols_dict = {'calls': [i for i in df.columns if 'calls' in i]\n",
        "                 , 'minutes': [i for i in df.columns if 'minutes' in i]\n",
        "                 , 'charge': [i for i in df.columns if 'charge' in i]\n",
        "                }\n",
        "\n",
        "    for k, v in cols_dict.items():\n",
        "        sums = df[v].sum(axis=1)\n",
        "        df[f'{k}_total'] = sums\n",
        "        for col in v:\n",
        "            df[f'pct_{col}'] = df[col] / sums\n",
        "\n",
        "    return df\n",
        "\n",
        "df = prepare_data()\n",
        "print(df.shape)\n",
        "df.head()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-09-03T20:42:59.657851Z",
          "iopub.execute_input": "2022-09-03T20:42:59.658227Z",
          "iopub.status.idle": "2022-09-03T20:42:59.747294Z",
          "shell.execute_reply.started": "2022-09-03T20:42:59.658194Z",
          "shell.execute_reply": "2022-09-03T20:42:59.746186Z"
        },
        "trusted": true,
        "id": "w7TTMNH9cW0q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define features and target\n",
        "X = df.drop(columns=['Churn'])\n",
        "y = df['Churn']\n",
        "print(f'X: {X.shape}\\ny: {y.shape}')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-09-03T20:42:59.75104Z",
          "iopub.execute_input": "2022-09-03T20:42:59.751413Z",
          "iopub.status.idle": "2022-09-03T20:42:59.759023Z",
          "shell.execute_reply.started": "2022-09-03T20:42:59.751381Z",
          "shell.execute_reply": "2022-09-03T20:42:59.758307Z"
        },
        "trusted": true,
        "id": "yd7z6fXjcW0r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Explore Data"
      ],
      "metadata": {
        "id": "J5iX8EapcW0s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Most customers are in West Virginia\n",
        "# Very few customers are in California\n",
        "(df[\n",
        "    [i for i in df if 'State' in i]\n",
        "]\n",
        " .sum()\n",
        " .sort_values()\n",
        " .plot(kind='barh'\n",
        "       , figsize=(5,20)\n",
        "      )\n",
        ")"
      ],
      "metadata": {
        "scrolled": true,
        "execution": {
          "iopub.status.busy": "2022-09-03T20:42:59.759992Z",
          "iopub.execute_input": "2022-09-03T20:42:59.760528Z",
          "iopub.status.idle": "2022-09-03T20:43:00.673034Z",
          "shell.execute_reply.started": "2022-09-03T20:42:59.760498Z",
          "shell.execute_reply": "2022-09-03T20:43:00.671954Z"
        },
        "trusted": true,
        "id": "E9OeLoOFcW0s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for 'Churn' class imbalance\n",
        "pct_churn = (y\n",
        "             .value_counts(normalize=True)\n",
        "             .loc[1]\n",
        "             * 100\n",
        "            )\n",
        "print(f'{pct_churn:.1f}% of customers churn')\n",
        "y.value_counts()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-09-03T20:43:00.674382Z",
          "iopub.execute_input": "2022-09-03T20:43:00.674686Z",
          "iopub.status.idle": "2022-09-03T20:43:00.686504Z",
          "shell.execute_reply.started": "2022-09-03T20:43:00.674657Z",
          "shell.execute_reply": "2022-09-03T20:43:00.685294Z"
        },
        "trusted": true,
        "id": "Es0-cQ3ccW0t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Correlation matrix with all the features\n",
        "plt.figure(figsize=(16,12))\n",
        "sns.heatmap(X.corr()\n",
        "            , cmap=\"RdBu_r\"\n",
        "            , vmin=-1\n",
        "            , vmax=1)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-09-03T20:43:00.688595Z",
          "iopub.execute_input": "2022-09-03T20:43:00.689146Z",
          "iopub.status.idle": "2022-09-03T20:43:02.015534Z",
          "shell.execute_reply.started": "2022-09-03T20:43:00.689085Z",
          "shell.execute_reply": "2022-09-03T20:43:02.014703Z"
        },
        "trusted": true,
        "id": "X4PwDWTfcW0t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Identify highly correlated features\n",
        "# (Taken from page 70 on http://floodkingz.org/INZEPOCKET/machinelearningpocketreference.pdf)\n",
        "def correlated_columns(df, threshold=0.95):\n",
        "    return (\n",
        "        df.corr()\n",
        "        .pipe(\n",
        "            lambda df1: pd.DataFrame(np.tril(df1, k=-1)\n",
        "                                     , columns=df.columns\n",
        "                                     , index=df.columns,\n",
        "                                    )\n",
        "        )\n",
        "        .stack()\n",
        "        .rename(\"correlation\")\n",
        "        .pipe(\n",
        "            lambda s: s[s.abs() > threshold]\n",
        "            .reset_index()\n",
        "        ).query(\"level_0 not in level_1\")\n",
        "    )\n",
        "\n",
        "colinears = correlated_columns(X)\n",
        "colinears.sort_values('correlation', ascending=False)"
      ],
      "metadata": {
        "scrolled": true,
        "execution": {
          "iopub.status.busy": "2022-09-03T20:43:02.016614Z",
          "iopub.execute_input": "2022-09-03T20:43:02.017646Z",
          "iopub.status.idle": "2022-09-03T20:43:02.230627Z",
          "shell.execute_reply.started": "2022-09-03T20:43:02.017607Z",
          "shell.execute_reply": "2022-09-03T20:43:02.229496Z"
        },
        "trusted": true,
        "id": "28UCTABkcW0u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove half of the highly correlated features\n",
        "colinear_cols = colinears['level_0'].tolist()\n",
        "X.drop(columns=colinear_cols, inplace=True)\n",
        "print(f'X: {X.shape}')"
      ],
      "metadata": {
        "scrolled": true,
        "execution": {
          "iopub.status.busy": "2022-09-03T20:43:02.231853Z",
          "iopub.execute_input": "2022-09-03T20:43:02.232164Z",
          "iopub.status.idle": "2022-09-03T20:43:02.240172Z",
          "shell.execute_reply.started": "2022-09-03T20:43:02.232136Z",
          "shell.execute_reply": "2022-09-03T20:43:02.238809Z"
        },
        "trusted": true,
        "id": "VOylicLicW0v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Correlation between the features and the target\n",
        "# (Most features have zero correlation with the target)\n",
        "with plt.style.context('ggplot'):\n",
        "    plt.figure(figsize=(10,6))\n",
        "    (df\n",
        "     .corr()\n",
        "     .loc[X.columns, y.name]\n",
        "     .sort_values(ascending=False)\n",
        "     .hist(alpha=0.3)\n",
        "    )\n",
        "    plt.xlabel(\"Correlation with 'Churn'\")\n",
        "    plt.ylabel('Frequency')\n",
        "    plt.title('Distribution of the correlation between the features and the target\\n(Most features have zero correlation with the target)')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-09-03T20:43:02.241726Z",
          "iopub.execute_input": "2022-09-03T20:43:02.242328Z",
          "iopub.status.idle": "2022-09-03T20:43:02.585034Z",
          "shell.execute_reply.started": "2022-09-03T20:43:02.24229Z",
          "shell.execute_reply": "2022-09-03T20:43:02.584196Z"
        },
        "trusted": true,
        "id": "m39pu2phcW0v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Explore categorical features\n",
        "# Plot the crosstab heatmap for each of the categorical features vs churn/non-churn\n",
        "# Use the chi-squared test to check whether there's a significant difference between the observed and expected frequencies\n",
        "X_cat = X.select_dtypes(exclude=[float, int])\n",
        "chi_t, chi_p = chi2(X_cat, y)\n",
        "chi_res = pd.Series(chi_p\n",
        "                    , index=X_cat.columns\n",
        "                    , name='Chi2_Pvalues')\n",
        "\n",
        "# Print most significant from the chi-squared test\n",
        "n_cat_imp = 5\n",
        "cat_imp = chi_res.sort_values().iloc[:n_cat_imp].index.tolist()\n",
        "print(f'In descending order, the {n_cat_imp} biggest differences between observed and expected values are:', *cat_imp, sep='\\n')\n",
        "\n",
        "# Plot\n",
        "n_rows = 12\n",
        "n_cols = 5\n",
        "fig, axes = plt.subplots(n_rows, n_cols, figsize=(20, 40))\n",
        "fig.suptitle('Heat Maps of the Crosstabs between each Categorical Feature and Churn\\nAnd the Chi-squared P-values\\n(X-axis is Churn and Y-axis is the Categorical Feature)')\n",
        "for i, ax in zip(range(X_cat.shape[1]), axes.flat):\n",
        "    to_plot = X_cat.iloc[:,i]\n",
        "    a = pd.crosstab(to_plot, y)\n",
        "    aa = a / a.sum().sum() # Counts --> percent of total (bc it's easier to understand)\n",
        "    sns.heatmap(aa\n",
        "                , cmap=\"RdBu_r\"\n",
        "                , annot=True\n",
        "                , fmt='.2f'\n",
        "                , vmin=0\n",
        "                , vmax=1\n",
        "                , ax=ax)\n",
        "    ax.set_title(f'{to_plot.name}: {chi_res.loc[to_plot.name]:.3f}')\n",
        "    ax.set_xlabel('')\n",
        "    ax.set_ylabel('')"
      ],
      "metadata": {
        "scrolled": true,
        "execution": {
          "iopub.status.busy": "2022-09-03T20:43:02.588287Z",
          "iopub.execute_input": "2022-09-03T20:43:02.588865Z",
          "iopub.status.idle": "2022-09-03T20:44:13.815587Z",
          "shell.execute_reply.started": "2022-09-03T20:43:02.588829Z",
          "shell.execute_reply": "2022-09-03T20:44:13.814497Z"
        },
        "trusted": true,
        "id": "FlWcmN1rcW0w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Explore continuous features\n",
        "# Plot the distributions of the continuous features\n",
        "# Segment the distributions by churn/non-churn\n",
        "# Balance the target classes so it's easier to understand the distribution of the feature for each of the classes\n",
        "# Use a t-test to measure whether there's a significant difference between the means (churn vs non-churn)\n",
        "rus = RandomUnderSampler(random_state=42)\n",
        "X_res, y_res = rus.fit_resample(X, y)\n",
        "X_cont = X_res.select_dtypes(include=[float, int])\n",
        "\n",
        "rus_df = pd.concat([X_cont, y_res], axis=1)\n",
        "x1 = rus_df[rus_df['Churn'] == 1].drop(columns=['Churn'])\n",
        "x2 = rus_df[rus_df['Churn'] == 0].drop(columns=['Churn'])\n",
        "ttest_stat, ttest_pval = ttest_ind(x1, x2)\n",
        "ttest_res = pd.Series(ttest_pval\n",
        "                      , name='Ttest_Pvalues'\n",
        "                      , index=x1.columns)\n",
        "\n",
        "# Print most significant from the t-test\n",
        "n_cont_imp = 5\n",
        "cont_imp = ttest_res.sort_values().iloc[:n_cont_imp].index.tolist()\n",
        "print(f'In descending order, the {n_cont_imp} biggest differences between the churn/non-churn means are:', *cont_imp, sep='\\n')\n",
        "\n",
        "# Plot\n",
        "n_rows = 5\n",
        "n_cols = 5\n",
        "fig, axes = plt.subplots(n_rows, n_cols, figsize=(20, 18))\n",
        "fig.suptitle('Distributions of each Continuous Feature and Segmented by Churn\\nAnd the T-test P-values')\n",
        "for i, ax in zip(range(X_cont.shape[1]), axes.flat):\n",
        "    to_plot = X_cont.iloc[:,i]\n",
        "    sns.histplot(x=to_plot\n",
        "                 , hue=y_res\n",
        "                 , element='poly'\n",
        "                 , ax=ax)\n",
        "    ax.set_title(f'{to_plot.name}: {ttest_res.loc[to_plot.name]:.3f}')\n",
        "    ax.set_xlabel('')\n",
        "    ax.set_ylabel('')"
      ],
      "metadata": {
        "scrolled": true,
        "execution": {
          "iopub.status.busy": "2022-09-03T20:44:13.817351Z",
          "iopub.execute_input": "2022-09-03T20:44:13.819473Z",
          "iopub.status.idle": "2022-09-03T20:44:18.154985Z",
          "shell.execute_reply.started": "2022-09-03T20:44:13.819423Z",
          "shell.execute_reply": "2022-09-03T20:44:18.153932Z"
        },
        "trusted": true,
        "id": "cX-Lx6o4cW0x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def scaler_func(data):\n",
        "    \"\"\"Scale the continuous features and add back the discrete features\"\"\"\n",
        "    cont_df = data.select_dtypes(include=[float, int])\n",
        "    dis_df = data.select_dtypes(exclude=[float, int])\n",
        "    scaler = StandardScaler()\n",
        "    sca = scaler.fit_transform(cont_df)\n",
        "    sca_df = pd.DataFrame(sca, columns=cont_df.columns, index=cont_df.index)\n",
        "    scaled_df = pd.concat([sca_df, dis_df], axis=1)\n",
        "    return scaled_df"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-09-03T20:44:18.156463Z",
          "iopub.execute_input": "2022-09-03T20:44:18.156911Z",
          "iopub.status.idle": "2022-09-03T20:44:18.164775Z",
          "shell.execute_reply.started": "2022-09-03T20:44:18.156866Z",
          "shell.execute_reply": "2022-09-03T20:44:18.163695Z"
        },
        "trusted": true,
        "id": "wboddl9ZcW0z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Parallel coordinates plotting to further understand how the continuous features relate to 'Churn'\n",
        "plt.figure(figsize=(20,5))\n",
        "ax = pd.plotting.parallel_coordinates(frame=scaler_func(df)\n",
        "                                      , class_column=y.name\n",
        "                                      , cols=df.select_dtypes(include=[float, int]).columns\n",
        "                                      , color=('#FF5733', '#6EFF33')\n",
        "                                     )\n",
        "plt.setp(ax.get_xticklabels(), rotation=30, horizontalalignment='right')\n",
        "plt.xlabel('Continuous Features')\n",
        "plt.ylabel('Scaled Value')\n",
        "plt.title('Parallel Coordinates between each of the Continous Features and Churn')\n",
        "plt.show()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-09-03T20:44:18.16632Z",
          "iopub.execute_input": "2022-09-03T20:44:18.167201Z",
          "iopub.status.idle": "2022-09-03T20:44:23.272775Z",
          "shell.execute_reply.started": "2022-09-03T20:44:18.167166Z",
          "shell.execute_reply": "2022-09-03T20:44:23.271779Z"
        },
        "trusted": true,
        "id": "yoijnqjucW0z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use all the features (categorical & continuous),\n",
        "# to fit a decision tree to understand which features are most useful in separating churn/non-churn customers\n",
        "tree = DecisionTreeClassifier()\n",
        "tree.fit(X, y)\n",
        "tree_imp = pd.Series(tree.feature_importances_\n",
        "                    , name='tree_imp'\n",
        "                    , index=X.columns)\n",
        "\n",
        "# Plot feature importance\n",
        "# The importance measures the average gain of purity by splits of a given variable (Gini importance)\n",
        "(tree_imp\n",
        " .sort_values()\n",
        " .plot(kind='barh'\n",
        "       , figsize=(5,20)\n",
        "       , title='Decision Tree Feature Importance'\n",
        "       , xlabel='Feature'\n",
        "       , ylabel='Importance')\n",
        ")"
      ],
      "metadata": {
        "scrolled": true,
        "execution": {
          "iopub.status.busy": "2022-09-03T20:44:23.274555Z",
          "iopub.execute_input": "2022-09-03T20:44:23.275234Z",
          "iopub.status.idle": "2022-09-03T20:44:24.7283Z",
          "shell.execute_reply.started": "2022-09-03T20:44:23.27519Z",
          "shell.execute_reply": "2022-09-03T20:44:24.727338Z"
        },
        "trusted": true,
        "id": "jrt4LK2HcW00"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit and plot a decision tree with the most important features to build intuition about customers that churn vs don't churn\n",
        "n_most_imp = 6\n",
        "most_imp = (tree_imp\n",
        "            .sort_values()\n",
        "            .iloc[-n_most_imp:]\n",
        "            .index\n",
        "            .tolist()\n",
        "           )\n",
        "\n",
        "tree_2 = DecisionTreeClassifier(max_depth=5) # Limit the depth so it's easier to understand\n",
        "tree_2.fit(X[most_imp], y)\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(40,20))\n",
        "plot_tree(tree_2\n",
        "          , feature_names=most_imp\n",
        "          , class_names=[str(i) for i in sorted(y.unique())]\n",
        "          , filled=True\n",
        "          , fontsize=16\n",
        "         )\n",
        "plt.title(f'Decision tree with the {n_most_imp} most important features', fontsize=20)\n",
        "plt.show()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-09-03T20:44:24.729432Z",
          "iopub.execute_input": "2022-09-03T20:44:24.729731Z",
          "iopub.status.idle": "2022-09-03T20:44:26.568559Z",
          "shell.execute_reply.started": "2022-09-03T20:44:24.729703Z",
          "shell.execute_reply": "2022-09-03T20:44:26.567416Z"
        },
        "trusted": true,
        "id": "l7CK4jXAcW01"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sample Data"
      ],
      "metadata": {
        "id": "iwD2b3iccW02"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Partition training and testing data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y\n",
        "                                                    , test_size=0.3\n",
        "                                                    , stratify=y\n",
        "                                                    , random_state=42\n",
        "                                                   )\n",
        "# Training data\n",
        "n_tr = len(X_train)\n",
        "pct_tr = (y_train\n",
        "          .value_counts(normalize=True)\n",
        "          .loc[1]\n",
        "          *100\n",
        "         )\n",
        "vc_tr = y_train.value_counts()\n",
        "print(f' -- Training data -- \\nSample size: {n_tr:,.0f}\\nPositive class: {pct_tr:,.1f}%\\n{vc_tr}\\n')\n",
        "\n",
        "\n",
        "# Testing data\n",
        "n_te = len(X_test)\n",
        "pct_te = (y_test\n",
        "          .value_counts(normalize=True)\n",
        "          .loc[1]\n",
        "          *100\n",
        "         )\n",
        "vc_te = y_test.value_counts()\n",
        "print(f' -- Testing data -- \\nSample size: {n_te:,.0f}\\nPositive class: {pct_te:,.1f}%\\n{vc_te}')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-09-03T20:44:26.570013Z",
          "iopub.execute_input": "2022-09-03T20:44:26.570378Z",
          "iopub.status.idle": "2022-09-03T20:44:26.586963Z",
          "shell.execute_reply.started": "2022-09-03T20:44:26.570347Z",
          "shell.execute_reply": "2022-09-03T20:44:26.586115Z"
        },
        "trusted": true,
        "id": "JDgkHjC5cW03"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Selection"
      ],
      "metadata": {
        "id": "GVuLEo6ncW03"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Model selection process:\n",
        "# Only use the training data from the train_test_split\n",
        "# Cross validate for a more robust assessment of model performance\n",
        "# Scale the cross validation training and testing data separately to prevent data leakage\n",
        "# Balance the target class in the cross validation training data in two steps:\n",
        "    # 1) Over-sample the minority class ('Churn'=1) with SMOTE-NC, which handles categorical and continuous features\n",
        "    # 2) Under-sample the orignal majority class ('Churn'=0) by removing Tomek links to increase the separation between classes\n",
        "# Fit the model with the cross validation training data\n",
        "# Score the model with the cross validation testing data\n",
        "# Measure the prediction error\n",
        "# (I'm mostly interested in the recall score, though I also included a few other metrics for overall performance)\n",
        "\n",
        "# Classification models\n",
        "models = [KNeighborsClassifier()\n",
        "          , LogisticRegression(random_state=123, solver='liblinear') # The default solver could not converge\n",
        "          , GaussianNB()\n",
        "          , RandomForestClassifier(random_state=123)\n",
        "          , LGBMClassifier(random_state=123)\n",
        "         ]\n",
        "\n",
        "X_cat_mask = [True if X[i].dtype == 'uint8' else False for i in X.columns] # Mask categorical features for the resampling\n",
        "n_folds = 8\n",
        "kf = KFold(n_splits=n_folds)\n",
        "res = {}\n",
        "for i, (train_index, test_index) in enumerate(kf.split(X_train)):\n",
        "    # Training data\n",
        "    X_train_fold = X_train.iloc[train_index]\n",
        "    y_train_fold = y_train.iloc[train_index]\n",
        "\n",
        "    # Testing data\n",
        "    X_test_fold = X_train.iloc[test_index]\n",
        "    y_test_fold = y_train.iloc[test_index]\n",
        "\n",
        "    # Scale the training and testing data separately\n",
        "    X_train_fold = scaler_func(X_train_fold)\n",
        "    X_test_fold = scaler_func(X_test_fold)\n",
        "\n",
        "    # Over-sample the minority class ('Churn'=1) in the training data\n",
        "    sm = SMOTENC(random_state=42, categorical_features=X_cat_mask) # Specify which features are categorical\n",
        "    X_train_fold, y_train_fold = sm.fit_resample(X_train_fold, y_train_fold)\n",
        "\n",
        "    # Under-sample the 'original' majority class ('Churn'=0) in the over-sampled training data by removing Tomek links\n",
        "    # (Tomek links are pairs of instances of opposite classes who are their own nearest neighbors)\n",
        "    tl = TomekLinks(sampling_strategy=[0])\n",
        "    X_train_fold, y_train_fold = tl.fit_resample(X_train_fold, y_train_fold)\n",
        "\n",
        "    for model in models:\n",
        "        # Fit the model with the training data\n",
        "        model.fit(X_train_fold, y_train_fold)\n",
        "\n",
        "        # Score the model with the testing data\n",
        "        y_pred = model.predict(X_test_fold)\n",
        "        y_pred_proba = model.predict_proba(X_test_fold)\n",
        "\n",
        "        # Measure the prediction error\n",
        "        recall = metrics.recall_score(y_test_fold, y_pred)\n",
        "        f1 = metrics.f1_score(y_test_fold, y_pred)\n",
        "        auc = metrics.roc_auc_score(y_test_fold, y_pred)\n",
        "        logloss = metrics.log_loss(y_test_fold, y_pred_proba)\n",
        "\n",
        "        # Record results\n",
        "        name = model.__class__.__name__\n",
        "        res[f'{name}_{i}'] = {'Model': name\n",
        "                              , 'Iter': i\n",
        "                              , 'Recall': recall\n",
        "                              , 'F1': f1\n",
        "                              , 'AUC': auc\n",
        "                              , 'Logloss': logloss\n",
        "                             }\n",
        "\n",
        "# Plot results\n",
        "res = pd.DataFrame.from_dict(res).T.reset_index(drop=True)\n",
        "(res\n",
        " .groupby('Model')\n",
        " .mean()\n",
        " .drop(columns=['Iter'])\n",
        " .style\n",
        " .highlight_max(color='lightgreen', subset=['Recall', 'F1', 'AUC']) # higher is better --> highlight_max\n",
        " .highlight_min(color='lightgreen', subset=['Logloss']) # lower is better --> highlight_min\n",
        " .format(precision=2)\n",
        ")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-09-03T20:44:26.588685Z",
          "iopub.execute_input": "2022-09-03T20:44:26.589418Z",
          "iopub.status.idle": "2022-09-03T20:45:06.455049Z",
          "shell.execute_reply.started": "2022-09-03T20:44:26.589385Z",
          "shell.execute_reply": "2022-09-03T20:45:06.453948Z"
        },
        "trusted": true,
        "id": "C3jHIGKhcW04"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Feature Importance"
      ],
      "metadata": {
        "id": "ZbdQ85MxcW04"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit the best model on all the data\n",
        "\n",
        "# Scale data\n",
        "X_ = scaler_func(X)\n",
        "\n",
        "# Over-sample the minority class ('Churn'=1)\n",
        "X_cat_mask = [True if X[i].dtype == 'uint8' else False for i in X.columns]\n",
        "sm = SMOTENC(random_state=42, categorical_features=X_cat_mask)\n",
        "X_, y_ = sm.fit_resample(X_, y)\n",
        "\n",
        "# Under-sample the 'original' majority class ('Churn'=0)\n",
        "tl = TomekLinks(sampling_strategy=[0])\n",
        "X_, y_ = tl.fit_resample(X_, y_)\n",
        "\n",
        "# Fit model\n",
        "clf = LGBMClassifier(random_state=123)\n",
        "clf.fit(X_, y_)\n",
        "\n",
        "# Feature importance\n",
        "feature_importances = pd.Series(clf.feature_importances_\n",
        "                               , name='imp'\n",
        "                               , index=X.columns)\n",
        "\n",
        "# Plot\n",
        "(feature_importances\n",
        " .sort_values()\n",
        " .plot(kind='barh', figsize=(8,24))\n",
        ")"
      ],
      "metadata": {
        "scrolled": true,
        "execution": {
          "iopub.status.busy": "2022-09-03T20:45:06.456473Z",
          "iopub.execute_input": "2022-09-03T20:45:06.45681Z",
          "iopub.status.idle": "2022-09-03T20:45:15.378142Z",
          "shell.execute_reply.started": "2022-09-03T20:45:06.45678Z",
          "shell.execute_reply": "2022-09-03T20:45:15.376762Z"
        },
        "trusted": true,
        "id": "CjbQQuykcW05"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Submission"
      ],
      "metadata": {
        "id": "FjgRrC2UcW05"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the smaller dataset for the final prediction\n",
        "df20 = pd.read_csv('/kaggle/input/telecom-churn-datasets/churn-bigml-20.csv')\n",
        "\n",
        "# Prepare data\n",
        "df20 = prepare_data(df=df20)\n",
        "\n",
        "# Features and target\n",
        "X20 = df20[X.columns]\n",
        "y20 = df20[y.name]\n",
        "\n",
        "# Scale features\n",
        "X20 = scaler_func(X20)\n",
        "\n",
        "# Prediction\n",
        "submission = clf.predict(X20)\n",
        "\n",
        "# Recall\n",
        "submission_recall = metrics.recall_score(y20, submission)\n",
        "print(f'Final submission recall: {submission_recall:.2f}')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-09-03T20:45:15.38088Z",
          "iopub.execute_input": "2022-09-03T20:45:15.381806Z",
          "iopub.status.idle": "2022-09-03T20:45:15.457175Z",
          "shell.execute_reply.started": "2022-09-03T20:45:15.381757Z",
          "shell.execute_reply": "2022-09-03T20:45:15.451782Z"
        },
        "trusted": true,
        "id": "vXiE88h6cW06"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Notebook runtime\n",
        "print(datetime.now() - ts)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-09-03T20:45:15.458676Z",
          "iopub.execute_input": "2022-09-03T20:45:15.45904Z",
          "iopub.status.idle": "2022-09-03T20:45:15.464824Z",
          "shell.execute_reply.started": "2022-09-03T20:45:15.459009Z",
          "shell.execute_reply": "2022-09-03T20:45:15.463658Z"
        },
        "trusted": true,
        "id": "Yqgzf9GhcW06"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}